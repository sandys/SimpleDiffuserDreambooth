{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XU7NuMAA2drw",
    "outputId": "7eb9b063-664f-4a42-e960-728ec9608c42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3090, 24576 MiB, 24267 MiB\n"
     ]
    }
   ],
   "source": [
    "#@markdown Check type of GPU and VRAM available.\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzM7j0ZSc_9c"
   },
   "source": [
    "https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnTMyW41cC1E"
   },
   "source": [
    "## Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aLWXPZqjsZVV"
   },
   "outputs": [],
   "source": [
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aLWXPZqjsZVV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq diffusers\n",
    "%pip install -q -U --pre triton\n",
    "%pip install -q accelerate==0.15.0 transformers ftfy bitsandbytes gradio natsort\n",
    "%pip install -q scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes==0.35\n",
      "  Downloading bitsandbytes-0.35.0-py3-none-any.whl (62.5 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62.5 MB 130.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "  Attempting uninstall: bitsandbytes\n",
      "    Found existing installation: bitsandbytes 0.35.4\n",
      "    Uninstalling bitsandbytes-0.35.4:\n",
      "      Successfully uninstalled bitsandbytes-0.35.4\n",
      "Successfully installed bitsandbytes-0.35.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes==0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LD_LIBRARY_PATH']='/usr/lib/x86_64-linux-gnu/:/opt/conda/lib/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ DEBUG INFORMATION +++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "++++++++++ POTENTIALLY LIBRARY-PATH-LIKE ENV VARS ++++++++++\n",
      "'JPY_SESSION_NAME': 'proj/275c07eb-95cc-4571-9fa2-45f5dfc2e0eb'\n",
      "'LD_LIBRARY_PATH': '/usr/lib/x86_64-linux-gnu/:/opt/conda/lib/'\n",
      "'MPLBACKEND': 'module://matplotlib_inline.backend_inline'\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "WARNING: Please be sure to sanitize sensible info from any such env vars!\n",
      "\n",
      "++++++++++++++++++++++++++ OTHER +++++++++++++++++++++++++++\n",
      "COMPILED_WITH_CUDA = True\n",
      "COMPUTE_CAPABILITIES_PER_GPU = ['8.6']\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++ DEBUG INFO END ++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Running a quick check that:\n",
      "    + library is importable\n",
      "    + CUDA function is callable\n",
      "\n",
      "SUCCESS!\n",
      "Installation was successful!\n"
     ]
    }
   ],
   "source": [
    "!python -m bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "y4lqqWT_uxD2"
   },
   "outputs": [],
   "source": [
    "#@title Login to HuggingFace ðŸ¤—\n",
    "\n",
    "#@markdown You need to accept the model license before downloading or using the Stable Diffusion weights. Please, visit the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5), read the license and tick the checkbox if you agree. You have to be a registered user in ðŸ¤— Hugging Face Hub, and you'll also need to use an access token for the code to work.\n",
    "# https://huggingface.co/settings/tokens\n",
    "!mkdir -p ~/.huggingface\n",
    "HUGGINGFACE_TOKEN = \"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\" #@param {type:\"string\"}\n",
    "!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfTlc8Mqb8iH"
   },
   "source": [
    "### Install xformers from precompiled wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cutlass\n",
      "  Downloading cutlass-0.0.1.tar.gz (64 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64 kB 7.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: cutlass\n",
      "  Building wheel for cutlass (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cutlass: filename=cutlass-0.0.1-py3-none-any.whl size=70843 sha256=4c6362a295584ae13249e06dd80c3f7f129c14fd26e5d3f3d4aadd2c90115ecd\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/cf/04/bdc024e49cdca5eebee0f463bcd9bd370933704eed50d1fe0a\n",
      "Successfully built cutlass\n",
      "Installing collected packages: cutlass\n",
      "Successfully installed cutlass-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting ninja\n",
      "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 145 kB 16.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: ninja\n",
      "Successfully installed ninja-1.11.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cutlass\n",
    "%pip install ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: xformers 0.0.15.dev0+affe4da.d20221210\n",
      "Uninstalling xformers-0.0.15.dev0+affe4da.d20221210:\n",
      "  Successfully uninstalled xformers-0.0.15.dev0+affe4da.d20221210\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall xformers -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6dcjPnnaiCn",
    "outputId": "ac7dc3db-27a2-4dd4-963e-4d77dc313a5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TORCH_CUDA_ARCH_LIST=8.6\n",
      "env: FORCE_CUDA=1\n",
      "env: XFORMERS_BUILD_TYPE=Release\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: MAX_JOBS=1\n",
      "env: DISTUTILS_USE_SDK=1\n",
      "Collecting xformers\n",
      "  Cloning https://github.com/facebookresearch/xformers to /tmp/pip-install-frmj4vdk/xformers_2bca26572d1b40c88deab033b4efb06a\n",
      "  Running command git clone -q https://github.com/facebookresearch/xformers /tmp/pip-install-frmj4vdk/xformers_2bca26572d1b40c88deab033b4efb06a\n",
      "  Resolved https://github.com/facebookresearch/xformers to commit ed9912521df673c64f681d263264acacc28060b8\n",
      "  Running command git submodule update --init --recursive -q\n",
      "Requirement already satisfied: torch>=1.12 in /opt/conda/lib/python3.9/site-packages (from xformers) (1.13.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from xformers) (1.22.3)\n",
      "Requirement already satisfied: pyre-extensions==0.0.23 in /opt/conda/lib/python3.9/site-packages (from xformers) (0.0.23)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.9/site-packages (from xformers) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from pyre-extensions==0.0.23->xformers) (4.4.0)\n",
      "Requirement already satisfied: typing-inspect in /opt/conda/lib/python3.9/site-packages (from pyre-extensions==0.0.23->xformers) (0.8.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from typing-inspect->pyre-extensions==0.0.23->xformers) (0.4.3)\n",
      "Building wheels for collected packages: xformers\n",
      "  Building wheel for xformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for xformers: filename=xformers-0.0.15.dev0+ed99125.d20221212-cp39-cp39-linux_x86_64.whl size=8882918 sha256=c0422cf9213c7bd37168dfc82d064dd71f2528c7fa86c6a18eb4aaafde76b891\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3j8ghkvm/wheels/f5/bb/92/4b854308ed1a6efaf8df6e4415ad50469e59721d6018d1b19b\n",
      "Successfully built xformers\n",
      "Installing collected packages: xformers\n",
      "Successfully installed xformers-0.0.15.dev0+ed99125.d20221212\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# If precompiled wheels don't work, install it with the following command.\n",
    "#Go to here, find your GPU (probably in the Quadro/RTX or GeForce/TITAN sections), and note the compute compatibility version number listed for your GPU.\n",
    "#https://developer.nvidia.com/cuda-gpus#compute\n",
    "#https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/3525#discussioncomment-3965024\n",
    "\n",
    "#https://github.com/Azadehkhojandi/PantryImageSegmentation/blob/043d2553238a8ce1805f3ef5856e26becfce3c3f/pytorch-mask-rcnn/.ipynb_checkpoints/InstallPyTorchSourceCUDA-checkpoint.ipynb\n",
    "#https://github.com/facebookresearch/xformers/blob/main/.github/workflows/wheels.yml\n",
    "\n",
    "%env TORCH_CUDA_ARCH_LIST = 8.6\n",
    "%env FORCE_CUDA = 1\n",
    "%env XFORMERS_BUILD_TYPE = Release\n",
    "%env CUDA_VISIBLE_DEVICES = 0 \n",
    "%env MAX_JOBS = 1\n",
    "%env DISTUTILS_USE_SDK = 1\n",
    "%pip install --no-clean git+https://github.com/facebookresearch/xformers#egg=xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0NV324ZcL9L"
   },
   "source": [
    "## Settings and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "def reset_folder(path):\n",
    "    if os.path.isdir(path):\n",
    "      shutil.rmtree(path)\n",
    "    os.makedirs(path,exist_ok=True)\n",
    "\n",
    "    \n",
    "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n",
    "#MODEL_NAME = \"CompVis/stable-diffusion-v1-4\"  #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTANCE = \"sukh02\"#@param {type:\"string\"}\n",
    "#INSTANCE_DIR = \"/content/data/\"+INSTANCE \n",
    "INSTANCE_DIR = \"data/zwx\"\n",
    "\n",
    "\n",
    "CLASS = \"person\" #@param {type:\"string\"}\n",
    "#CLASS_DIR = \"/content/data/\"+CLASS\n",
    "CLASS_DIR = \"data/person\"\n",
    "\n",
    "\n",
    "#OUTPUT_DIR = \"/content/stable_diffusion_weights/\" + INSTANCE\n",
    "OUTPUT_DIR = \"data/stable_diffusion_weights/zwx2\" #@param {type:\"string\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_folder(INSTANCE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_folder(CLASS_DIR)\n",
    "reset_folder(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn5ILIyDJIcX"
   },
   "source": [
    "# Start Training\n",
    "\n",
    "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
    "\n",
    "\n",
    "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
    "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
    "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
    "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
    "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
    "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
    "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
    "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
    "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
    "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
    "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ioxxvHoicPs"
   },
   "source": [
    "Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
    "\n",
    "remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n",
    "\n",
    "remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "id": "32gYIDDR1aCp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Stable-Diffusion-Regularization-Images-person_ddim'...\n",
      "remote: Enumerating objects: 1503, done.\u001b[K\n",
      "remote: Total 1503 (delta 0), reused 0 (delta 0), pack-reused 1503\u001b[K\n",
      "Receiving objects: 100% (1503/1503), 657.41 MiB | 17.86 MiB/s, done.\n",
      "Checking out files: 100% (1500/1500), done.\n"
     ]
    }
   ],
   "source": [
    "#@markdown download regularization data\n",
    "dataset=\"person_ddim\" #@param [\"man_euler\", \"man_unsplash\", \"person_ddim\", \"woman_ddim\", \"blonde_woman\"]\n",
    "!git clone  https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-{dataset}.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "32gYIDDR1aCp"
   },
   "outputs": [],
   "source": [
    "!mkdir -p data/person\n",
    "!mv -v Stable-Diffusion-Regularization-Images-{dataset}/{dataset}/*.* data/person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjcSXTp-u-Eg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "/opt/conda/lib/python3.9/site-packages/accelerate/accelerator.py:321: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...\n",
      "/opt/conda/lib/python3.9/site-packages/diffusers/configuration_utils.py:195: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "Caching latents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [01:34<00:00, 15.89it/s]\n",
      "Steps:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1685/2000 [10:54<02:01,  2.59it/s, loss=0.29, lr=1e-6]"
     ]
    }
   ],
   "source": [
    "!accelerate launch train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
    "  --instance_data_dir=$INSTANCE_DIR \\\n",
    "  --class_data_dir=$CLASS_DIR \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --instance_prompt=$INSTANCE \\\n",
    "  --class_prompt=$CLASS\\\n",
    "  --seed=1337 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --train_text_encoder \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --use_8bit_adam \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --gradient_checkpointing \\\n",
    "  --learning_rate=1e-6 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --num_class_images=1500 \\\n",
    "  --sample_batch_size=4 \\\n",
    "  --max_train_steps=2000 \\\n",
    "  --save_interval=10_000 \\\n",
    "  --save_sample_prompt=$CLASS\n",
    "\n",
    "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
    "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V8wgU0HN-Kq",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Convert weights to ckpt to use in web UIs like AUTOMATIC1111."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dcXzsUyG1aCy"
   },
   "outputs": [],
   "source": [
    "#@markdown Run conversion.\n",
    "ckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n",
    "\n",
    "half_arg = \"\"\n",
    "#@markdown  Whether to convert to fp16, takes half the space (2GB).\n",
    "fp16 = True #@param {type: \"boolean\"}\n",
    "if fp16:\n",
    "    half_arg = \"--half\"\n",
    "!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n",
    "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToNG4fd_dTbF"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "referenced_widgets": [
      "21d3153693b0442bb0afe46738c9d9ae"
     ]
    },
    "id": "K6xoHWSsbcS3",
    "outputId": "75fb2672-a0a4-4149-ef0d-42ff0c247449"
   },
   "outputs": [],
   "source": [
    "#@title Run for generating images.\n",
    "import random\n",
    "import torch\n",
    "seed_value= 0\n",
    "random.seed(seed_value)\n",
    "\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, DiffusionPipeline, DPMSolverMultistepScheduler,EulerDiscreteScheduler\n",
    "\n",
    "from diffusers.models import AutoencoderKL, UNet2DConditionModel\n",
    "from IPython.display import display\n",
    "\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "import os\n",
    "model_path = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]  # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive          \n",
    "\n",
    "# unet_model = UNet2DConditionModel(\n",
    "#     sample_size=512 // 8,\n",
    "#     in_channels=4,\n",
    "#     out_channels=4,\n",
    "#     down_block_types=(\"DownBlock2D\",\"DownBlock2D\",\"CrossAttnDownBlock2D\",\"DownBlock2D\"),\n",
    "#     up_block_types=(\"CrossAttnUpBlock2D\",\"UpBlock2D\",\"UpBlock2D\",\"UpBlock2D\"),\n",
    "#     block_out_channels=(320,640,1280,1280),\n",
    "#     layers_per_block=2,\n",
    "#     cross_attention_dim=768,\n",
    "#     attention_head_dim=8,\n",
    "#     use_linear_projection=False\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "g_cuda = torch.Generator(device='cuda').manual_seed(seed_value)\n",
    "\n",
    "# unet_model.load_state_dict(torch.load(OUTPUT_DIR, map_location=device)[\"state_dict\"])\n",
    "\n",
    "#scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "#scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "#scheduler = DPMSolverMultistepScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "scheduler = EulerDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
    "\n",
    "\n",
    "#pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
    "#pipe =  DiffusionPipeline.from_pretrained(model_path, scheduler=scheduler,  torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "#https://github.com/huggingface/diffusers/blob/896c98a2aedaa25a9b47c6b4f9cafd7b3f7f0f54/scripts/convert_original_stable_diffusion_to_diffusers.py#L246\n",
    "\n",
    "\n",
    "#@markdown Can set random seed here for reproducibility.\n",
    "#g_cuda = torch.Generator(device='cuda').manual_seed(-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prompt = \"cinematic photography of pregnant sukh02 person with the body of christina hendricks in a green bikini, intricate, elegant, highly detailed, smooth, sharp focus, symmetrical face, fine details, masterpiece, trending on artstation, 4 k hdr 3 5 mm photography \"\n",
    "prompt2 = 'symmetrical!! highly detailed upper body photograph of sukh02 person with the body of christina hendricks wearing a corset in bladerunner, looking at the camera!! shiny oily skin, sweaty, zeiss lens, canon eos, unreal engine, redshift, octane render, global illumination, radiant light, detailed and intricate '\n",
    "#prompt = \"glamorous and sexy sukh02 person in lingerie beautiful, pearlescent skin, natural beauty, seductive eyes and face, elegant girl, natural beauty, very detailed face, seductive lady, full body portrait, natural lights, photorealism, summer vibrancy, cinematic, a portrait by artgerm, rossdraws, Norman Rockwell, magali villeneuve, Gil Elvgren, Alberto Vargas, Earl Moran, Enoch Bolles\"\n",
    "\n",
    "# p0 = 'sukh02 person profile picture by margaret keane, dynamic pose, intricate, futuristic, fantasy, elegant, by stanley artgerm lau, greg rutkowski, thomas kindkade, alphonse mucha, loish, norman rockwell'\n",
    "#p1 = 'Realistic detailed face portrait of a beautiful futuristic italian renaissance queen in opulent alien glass armor by alphonse mucha, sukh02 person, ayami kojima, amano, greg hildebrandt, and mark brooks, female, feminine, art nouveau, ornate italian renaissance cyberpunk, iridescent venetian blown glass, neo - gothic, gothic, character concept design'\n",
    "p2 = 'Photo of a gorgeous sukh02 person in the style of stefan kostic, realistic, sharp focus, 8 k high definition, insanely detailed, intricate, elegant, art by david cronenberg and stanley lau and artgerm'\n",
    "p3 = 'Photo of a gorgeous female in the style of stefan kostic, sukh02 person, realistic, half body shot, sharp focus, 8 k high definition, insanely detailed, intricate, elegant, art by stanley lau and artgerm, extreme blur cherry blossoms background'\n",
    "# p4 = 'God and goddess, XXX, white hair, long hair, gorgeous, amazing, elegant, intricate, highly detailed, digital painting, artstation, concept art, sharp focus, illustration, art by artgerm and greg rutkowski and alphonse mucha'\n",
    "p5 = 'Portrait of  military engineer sukh02 person, with short white hair,  wearing overalls, medium shot, portrait, concept art, natural lighting, illustration, full color, highly detailed, photorealistic, by greg rutkowski, artstation'\n",
    "# bank = [p0, p1, p2, p3, p4, p5]\n",
    "\n",
    "\n",
    "#prompt = \"photo of zwx dog\"\n",
    "#negative_prompt = \"((painting)), ((frame)), ((drawing)), ((sketch)), ((camera)), ((rendering)),(((cropped))), (((watermark))), ((logo)), ((barcode)), ((UI)), ((signature)), ((text)), ((label)), ((error)), ((title)), stickers, markings, speech bubbles, lines, cropped, lowres, low quality, artifacts, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\" #@param {type:\"string\"}\n",
    "negative_prompt = \"\"\n",
    "num_samples = 1 #@param {type:\"number\"}\n",
    "guidance_scale = 10 #@param {type:\"number\"}\n",
    "num_inference_steps = 30 #@param {type:\"number\"}\n",
    "height = 768 #@param {type:\"number\"}\n",
    "width = 512 #@param {type:\"number\"}\n",
    "\n",
    "with autocast(\"cuda\"), torch.inference_mode():\n",
    "    pipe =  DiffusionPipeline.from_pretrained(model_path, force_download=True,safety_checker=None,custom_pipeline=\"lpw_stable_diffusion\", scheduler=scheduler,  torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "    images = pipe(\n",
    "        prompt2,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=num_samples,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        #generator=g_cuda\n",
    "    ).images\n",
    "\n",
    "for img in images:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/huggingface_hub/file_download.py:597: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  FutureWarning,\n",
      "You have disabled the safety checker for <class 'diffusers_modules.git.lpw_stable_diffusion.StableDiffusionLongPromptWeightingPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               | 14/50 [00:06<00:14,  2.51it/s]"
     ]
    }
   ],
   "source": [
    "!python infer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "referenced_widgets": [
      "21d3153693b0442bb0afe46738c9d9ae"
     ]
    },
    "id": "K6xoHWSsbcS3",
    "outputId": "75fb2672-a0a4-4149-ef0d-42ff0c247449"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1460/568581680.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "for i, img in enumerate(images):\n",
    "    img.save('images/'+str(i).zfill(4)+'.jpg')\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "WMCqQ5Tcdsm2"
   },
   "outputs": [],
   "source": [
    "#@markdown Run Gradio UI for generating images.\n",
    "import gradio as gr\n",
    "\n",
    "def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n",
    "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
    "        return pipe(\n",
    "                prompt, height=int(height), width=int(width),\n",
    "                negative_prompt=negative_prompt,\n",
    "                num_images_per_prompt=int(num_samples),\n",
    "                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
    "                generator=g_cuda\n",
    "            ).images\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n",
    "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
    "            run = gr.Button(value=\"Generate\")\n",
    "            with gr.Row():\n",
    "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
    "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
    "            with gr.Row():\n",
    "                height = gr.Number(label=\"Height\", value=512)\n",
    "                width = gr.Number(label=\"Width\", value=512)\n",
    "            num_inference_steps = gr.Slider(label=\"Steps\", value=50)\n",
    "        with gr.Column():\n",
    "            gallery = gr.Gallery()\n",
    "\n",
    "    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
